import concurrent.futures
import itertools
import pathlib
import os

import click
import numpy as np
import pandas as pd

import xfirst.config as config
import xfirst.data as data
import xfirst.profile_functions as profile_functions
import xfirst.util as util

def make_fits(
  datadir: str | os.PathLike,
  workers: int | None = None,
  verbose: bool = True,
):
  """
  Reads profiles from the given directory and fits all using a universal shower
  profile (USP) function.
  
  The USP function is simply a reparametrization of the usual Gaisser-Hillas
  profile, but with the advantage of the shape parameters presenting a more
  well-behaved distribution.

  This function can perform in parallel by setting the argument workers to the
  desired number of parallel threads or leaving it as the default None, in which
  case it will be converted to the value of `os.cpu_count()`.

  Parameters
  ----------
  datadir : str | os.PathLike
    directory containing a "profiles" subdirectory generated by the
   `make_datasets` function. All profiles under such directory will be fitted.

  workers : int, optional
    number of jobs to run in parallel. If not defined, will be converted to the
    value returned by `os.cpu_count()`

  verbose : bool, default True
    if true, information will be printed as data is processed.
  """
  basedir = pathlib.Path(datadir).resolve()
  batches = workers or os.cpu_count() or 1
  function = profile_functions.usp

  util.echo(verbose, f'generating fits for cut configurations {[c.name for c in config.cuts]}')

  for c in config.cuts:
    util.echo(verbose, f'\nprocessing cut configuration {c.name}')

    for d, p in itertools.product(config.datasets, config.particles):
      
      profiles, depths = data.load_profiles(datadir = basedir, cut = c, datasets = d, particles = p).values()

      ys = [y.to_numpy() for y in util.split(profiles, batches = batches)]
      xs = [itertools.repeat(np.copy(depths), len(y)) for y in ys]
      fs = [function() for _ in ys]

      with concurrent.futures.ProcessPoolExecutor(batches) as exec:
        fits = exec.map(function.get_fits, fs, xs, ys)
        fits = pd.concat(fits, ignore_index = True, copy = False)
        fits.index.name = 'id'
        # update status by applying cuts
        fits.status = data.get_good_fits(fits, c).astype(fits.status.dtype)
        # save
        util.hdf_save(basedir/f'fits/range-{c.min_depth}-{c.max_depth}/{d}', fits, p, verbose)

@click.command()
@click.option('--datadir', type = click.Path(exists = True), required = True)
@click.option('--workers', type = click.IntRange(1, 2048), default = None, required = False)
@click.option('--verbose/--no-verbose', default = True)
def main(**kwargs):
  make_fits(**kwargs)
  return 0

if __name__ == '__main__':
  main()
