#!/bin/bash
#SBATCH      --job-name mlpfit
#SBATCH         --nodes 2 #? must match the number of layer configurations
#SBATCH --cpus-per-task 48
#SBATCH     --partition sequana_cpu_shared
#SBATCH          --time 96:00:00
#SBATCH     --mail-type ALL
#SBATCH     --mail-user luan.arbeletche@gmail.com
#SBATCH        --output /dev/null #? do not change
#SBATCH         --error /dev/null #? do not change
#SBATCH     --exclusive           #? do not change

source /scratch/astroparti/luan.arbeletche/ml-conex-xfirst/sd-scripts/prepare_env.sh

submit "${@}"

for layers in '64-64-64' '32-32-32'; do
  echo "launching layer configuration ${layers}"
  srun -N1 -n1 -o "${LOGS_DIR}/%x_job_%J.out" -e "${LOGS_DIR}/%x_job_%J.err" \
    python "${SCRIPTS_DIR}/train_mlp_fit.py" \
    --datadir "${DATA_DIR}" \
    --layers "${layers}" \
    --save "${MODELS_DIR}" \
    --nshowers train "${NTRAIN}" \
    --nshowers validation "${NVAL}" \
    --nshowers test "${NTEST}" \
    &
done

wait
